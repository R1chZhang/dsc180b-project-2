{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade05b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import distance transformer\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "dist_transform = T.Distance()\n",
    "\n",
    "# Import KNN transformer\n",
    "knn_transform = T.KNNGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00cc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(file_path,skip=0):\n",
    "    fp=file_path\n",
    "    skip_lines = skip #default=0\n",
    "          \n",
    "    with open(fp, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Organize the document by label\n",
    "    #labels = re.findall(r'\\b\\w+\\s\\w+\\b', text)\n",
    "    current_label = None\n",
    "    labels = {}\n",
    "\n",
    "    for i,line in enumerate(lines):\n",
    "        if i<skip_lines:\n",
    "            continue\n",
    "            \n",
    "        # match labels\n",
    "        match = re.search(r'^([^\\d\\s]+(?:\\s+[^\\d\\s]+)*)', line)\n",
    "        tokens = line.strip().split()\n",
    "        \n",
    "        if match:\n",
    "            current_label = match.group(1).strip()\n",
    "            labels[current_label] = []\n",
    "        \n",
    "        elif len(tokens) == 1:  # Instance index\n",
    "            labels[current_label].append(int(tokens[0]))\n",
    "\n",
    "    # Build a mapping from instance index to label\n",
    "    mapping = {}\n",
    "    for label, indices in labels.items():\n",
    "        for index in indices:\n",
    "            mapping[index] = label\n",
    "            \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881d3e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_off(file_path):\n",
    "    \"\"\"\n",
    "    input: the filepath of the .off file\n",
    "    output: the Data object after parsing that file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Read the header and get the number of vertices and faces\n",
    "        f.readline()#skip 1st line\n",
    "        header = f.readline().strip().split(' ')\n",
    "        num_vertices = int(header[0])\n",
    "        num_faces = int(header[1])\n",
    "        \n",
    "        # Read the vertices\n",
    "        vertices = []\n",
    "        for i in range(num_vertices):\n",
    "            vertex = list(map(float, f.readline().strip().split(' ')))\n",
    "            vertices.append(vertex)\n",
    "        vertices = torch.tensor(vertices)        \n",
    "        \n",
    "        # Read the faces and build the edges\n",
    "        edges = []\n",
    "        for i in range(num_faces):\n",
    "            face = list(map(int, f.readline().strip().split(' ')[1:]))\n",
    "            for j in range(len(face)):\n",
    "                edge = (face[j], face[(j+1)%len(face)])\n",
    "                edges.append(edge)\n",
    "        edges = torch.tensor(edges, dtype=torch.long)\n",
    "        \n",
    "        #Pad and trim to match dimensionality\n",
    "        #num_nodes = max(edges.max().item() + 1, vertices.size(0))\n",
    "        #new_x = torch.zeros((num_nodes, vertices.size(1)))\n",
    "        #new_x[:vertices.size(0), :] = vertices\n",
    "        \n",
    "        # creating positional matrix\n",
    "        #pos = []\n",
    "        #for i in range(num_vertices):\n",
    "        #    pos.append([float(x) for x in f.readline().split()])\n",
    "        pos = torch.tensor(vertices, dtype=torch.float)\n",
    "        \n",
    "        return Data(x=vertices, edge_index=edges.transpose(0,1),pos=pos)\n",
    "\n",
    "def run_parse(root_fp, cla_fp):\n",
    "    # Set the root folder that contains all the subfolders of .off files\n",
    "    root_folder = root_fp\n",
    "    \n",
    "    out_dict = make_labels(cla_fp)\n",
    "    ulab = np.unique(list(out_dict.values()))\n",
    "    \n",
    "    #  Label Encoder\n",
    "    le = LabelEncoder()\n",
    "    le.fit(ulab.reshape(-1, 1))\n",
    "\n",
    "    # Get a mapping of idx:lab and generate label class\n",
    "    out_dict = make_labels(cla_fp)\n",
    "    ulab = np.unique(list(out_dict.values()))\n",
    "\n",
    "    #Parsing all files and build a list of graph data objects\n",
    "    graphs = []\n",
    "    for filename in os.listdir(root_folder):\n",
    "        if filename.endswith('.off'):#Check file type\n",
    "            file_index = int(filename.split(\".\")[0])\n",
    "            if out_dict.get(file_index): #check index included in classification\n",
    "                filepath = os.path.join(root_folder,filename)\n",
    "                graph = parse_off(filepath)\n",
    "                file_label = le.transform([out_dict[file_index]])\n",
    "                graph.y = torch.tensor(file_label[0])\n",
    "                #add edge_attr\n",
    "                graph = dist_transform(graph)\n",
    "                #graph = knn_transform(graph)\n",
    "                graphs.append(graph)\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call dataloader directly\n",
    "#loader = DataLoader(graphs, batch_size= 16, drop_last=True)\n",
    "#torch.save(loader,'psb_loader')#this loader does not have attributes like num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd8762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "    def __init__(self, data_list):\n",
    "        super().__init__()\n",
    "        self.data_list = data_list\n",
    "        \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        num_classes = [data.y for data in self.data_list]\n",
    "        return len(torch.unique(torch.tensor(num_classes)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Module\n",
    "graphs = run_parse(root_fp='...',cla_fp='...')\n",
    "len(graphs)\n",
    "psb_set = MyDataset(graphs)\n",
    "torch.save(psb_set,'...')\n",
    "#psb_set = torch.load('psb.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653e700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237123e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
